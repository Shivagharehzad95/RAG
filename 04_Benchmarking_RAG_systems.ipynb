{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa135da3",
   "metadata": {},
   "source": [
    "## Key Metrics\n",
    "| Metric                       | What it Measures                       | Why it Matters                           |\n",
    "| ---------------------------- | -------------------------------------- | ---------------------------------------- |\n",
    "| **Latency**                  | Time to retrieve and generate          | Affects user experience and scaling      |\n",
    "| **Recall\\@k / Precision\\@k** | Did the right doc(s) appear in top-k?  | Validates retriever quality              |\n",
    "| **Context Relevance**        | Is the context useful for the LLM?     | Determines if the LLM has what it needs  |\n",
    "| **Answer Accuracy**          | Is the final answer factually correct? | Measures end-to-end system effectiveness |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44879b76",
   "metadata": {},
   "source": [
    "## RAG Benchmarking Checklist: What to Measure and Why\n",
    "\n",
    "### 1. Embeddings\n",
    "\n",
    "Are we generating useful representations of the content and the query?\n",
    "\n",
    "- What to benchmark:\n",
    "    - **Embedding coverage**: Are you embedding all the content or losing some due to truncation/errors?\n",
    "    - **Semantic quality**: Do similar chunks/documents yield similar embeddings?\n",
    "    - **Query–document alignment**: Are queries embedded in the same space effectively as documents?\n",
    "- How to test:\n",
    "    - Visualize embedding space (e.g. with t-SNE or UMAP).\n",
    "    - Run nearest neighbor searches on known similar chunks.\n",
    "    - Use test queries and check if expected documents are close.\n",
    "\n",
    "### 2. Retriever\n",
    "\n",
    "Are we retrieving the correct and useful content chunks?\n",
    "\n",
    "- What to benchmark:\n",
    "    - **Recall@k / Precision@k**: Are the top-k retrieved chunks relevant?\n",
    "    - **Latency**: How fast is retrieval?\n",
    "    - **Filtering/metadata match**: Are filters (e.g. date, source) respected?\n",
    "    - **Chunk quality**: Are the retrieved chunks self-contained and understandable?\n",
    "- How to test:\n",
    "    - Create a small gold dataset of queries with expected docs.\n",
    "    - Log top-k retrieval results for human inspection.\n",
    "    - Use keyword matching or semantic similarity as automated checks.\n",
    "\n",
    "### 3. Generator (LLM)\n",
    "\n",
    "Does the answer contain the necessary information? Is it fluent, factual, concise?\n",
    "\n",
    "- What to benchmark:\n",
    "    - **Answer groundedness**: Is the generated answer based on retrieved content?\n",
    "    - **Factual accuracy**: Is it correct and non-hallucinated?\n",
    "    - **Fluency & structure**: Is it clear and readable?\n",
    "    - **Length control**: Is the output appropriately concise or detailed?\n",
    "- How to test:\n",
    "    - Compare answer to a reference answer (automatically or manually).\n",
    "    - Use LLM-assisted QA evaluation (e.g. LangChain’s QAEvalChain).\n",
    "    - Use human scoring rubrics (0–1 or 1–5 scale).\n",
    "\n",
    "### 4. System Integration\n",
    "\n",
    "Does the system perform consistently end-to-end?\n",
    "\n",
    "- End-to-end latency\n",
    "- Throughput (docs/sec indexed, QPS handled)\n",
    "- Fallback behavior: What happens when retrieval fails?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a2561",
   "metadata": {},
   "source": [
    "## Things that have an impact on the system\n",
    "\n",
    "| Component                   | Why It Matters                                         |\n",
    "| --------------------------- | ------------------------------------------------------ |\n",
    "| **Pre-processing quality**  | Poor chunking/tokenization affects embedding quality   |\n",
    "| **Chunk size sensitivity**  | Retrieval quality is heavily impacted by chunk size    |\n",
    "| **Indexing strategy**       | HNSW vs Flat vs IVF, impacts speed & recall            |\n",
    "| **Reranking effectiveness** | Reordering top-k with a smarter model improves quality |\n",
    "| **Prompt template design**  | Generation can vary wildly based on prompt structure   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00bf56c",
   "metadata": {},
   "source": [
    "## Research Frameworks & Benchmarking Tools\n",
    "\n",
    "- [BERGEN: A Benchmarking Library for RAG](https://arxiv.org/abs/2407.01102)\n",
    "Introduces an open-source library designed to standardize RAG evaluation with support for multiple retrievers, rerankers, LLMs, and metrics. Ideal for reproducible experiments.\n",
    "\n",
    "- [RAGBench: Explainable Benchmark for RAG Systems](https://arxiv.org/abs/2407.11005)\n",
    "Offers a labeled dataset (100k examples across industry-specific domains) and the TRACe framework—providing actionable, explainable evaluation metrics for RAG pipelines.\n",
    "\n",
    "- [RAGAS: Automated Evaluation of RAG](https://arxiv.org/abs/2309.15217)\n",
    "Proposes a framework for reference-free evaluation of RAG, aiming to assess retrieval and generation quality without requiring ground-truth annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e31253",
   "metadata": {},
   "source": [
    "## Domain-Specific Use Cases\n",
    "\n",
    "- [MIRAGE: RAG Benchmarking for Medicine](https://aclanthology.org/2024.findings-acl.372/)\n",
    "A healthcare-focused benchmark with over 7,600 questions, evaluating various combinations of corpora, retrievers, and LLMs for RAG in medical settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19660f51",
   "metadata": {},
   "source": [
    "## Practical Guides & Best Practices\n",
    "\n",
    "- \"RAG Evaluation Metrics: Assessing Answer Relevancy, Faithfulness, and More\" [(Confident AI blog)](https://www.confident-ai.com/blog/rag-evaluation-metrics-answer-relevancy-faithfulness-and-more): Breaks down core metrics such as contextual recall/precision, answer relevancy and faithfulness, and introduces the DeepEval open-source toolkit for streamlined implementation.\n",
    "\n",
    "- \"Evaluating RAG Pipelines\" [(Neptune AI Blog)](https://neptune.ai/blog/evaluating-rag-pipelines)\n",
    "\n",
    "Stresses multi-dimensional evaluation retriever-based (Recall@k, Precision@k, MRR), generator-based (citation precision/recall, token-level F1), plus cost and latency considerations.\n",
    "\n",
    "- \"Best Practices for RAG Evaluation\" [(Patronus AI)](https://www.patronus.ai/llm-testing/rag-evaluation-metrics)\n",
    "\n",
    "Identifies key metrics like context relevance/sufficiency and answer relevance/correctness, and emphasizes CI integration, continuous monitoring, and evolving gold standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50902794",
   "metadata": {},
   "source": [
    "## Advanced Benchmarking Platforms\n",
    "\n",
    "- [BenchmarkQED (Microsoft Research)](https://www.microsoft.com/en-us/research/blog/benchmarkqed-automated-benchmarking-of-rag-systems/)\n",
    "\n",
    "Introduces an end-to-end, automated benchmarking suite for RAG (including query generation, evaluation, and dataset prep), particularly aligned with GraphRAG-style retrieval.\n",
    "\n",
    "- [EvidentlyAI: 7 RAG Benchmarks](https://www.evidentlyai.com/blog/rag-benchmarks)\n",
    "\n",
    "A summary of various RAG benchmarks—including the Needle-in-a-Haystack test—helping measure retrieval accuracy under challenging scenarios."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
