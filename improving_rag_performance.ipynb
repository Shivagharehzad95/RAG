{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving RAG Performance\n",
    "\n",
    "This notebook walks through practical techniques to improve the performance of Retrieval-Augmented Generation (RAG) systems:\n",
    "\n",
    "- Updating Chunk Size\n",
    "- Re-Ranking Retrieved Chunks\n",
    "- Query Transformations\n",
    "- Fine-tuning with LoRA and QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Chunk Size\n",
    "\n",
    "Chunk size plays a critical role in retrieval accuracy and contextual relevance. Too small, and you may lose coherence; too large, and you risk irrelevance or hallucination. Research, such as [“Rethinking Chunk Size for Long-Document Retrieval”](https://arxiv.org/abs/2505.21700), shows that shorter chunks (~64–128 tokens) work better for factual, pinpoint queries, while larger chunks (512–1024 tokens) benefit broader context questions.\n",
    "arXiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac90f96",
   "metadata": {},
   "source": [
    "\n",
    "### Toolkit & Examples\n",
    "LlamaIndex offers a module ([Response Evaluation](https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)) to empirically identify an optimal chunk size for your dataset.\n",
    "\n",
    "\n",
    "[NVIDIA’s blog](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses/) also highlights real-world experimentation linking chunking strategy to retrieval accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Ranking Retrieved Chunks\n",
    "\n",
    "Initial retrieval (e.g., via vector search) may miss the most contextually relevant passages. A cross-encoder or LLM-based re-ranker can reorder those top-k candidates to surface higher-quality context before generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025f833",
   "metadata": {},
   "source": [
    "### Toolkit & Examples\n",
    "You can use a cross-encoder like sentence-transformers/cross-encoder/ms-marco-MiniLM-L-6-v2 to compute relevance scores between query and documents, then rerank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with SentenceTransformers CrossEncoder\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "query = \"What is the sick leave policy?\"\n",
    "retrieved_docs = [\n",
    "    \"Employees are entitled to 5 days of paid sick leave...\",\n",
    "    \"The company hosts an annual wellness seminar...\",\n",
    "    \"You need a doctor’s note for medical leave.\"\n",
    "]\n",
    "\n",
    "scores = model.predict([[query, doc] for doc in retrieved_docs])\n",
    "reranked = sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for doc, score in reranked:\n",
    "    print(f\"Score: {score:.2f} | Doc: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Transformations\n",
    "\n",
    "User questions can be vague or colloquial. Rewriting them into clearer, semantically enriched queries can improve retrieval precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fdb91",
   "metadata": {},
   "source": [
    "\n",
    "### Toolkit & Examples\n",
    "A simple LLM-based rephraser (e.g., a FLAN-T5 model) can be used to rephrase queries before sending to the retriever layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an LLM to rephrase a question\n",
    "from transformers import pipeline\n",
    "\n",
    "rephraser = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "query = \"What do I do if I'm sick?\"\n",
    "rephrased = rephraser(f\"Rephrase the question: {query}\", max_length=64, do_sample=False)\n",
    "\n",
    "print(\"Original:\", query)\n",
    "print(\"Rephrased:\", rephrased[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning LLMs with LoRA / QLoRA\n",
    "\n",
    "Fine-tuning full LLMs is expensive and resource-heavy. LoRA (Low-Rank Adaptation) introduces trainable adapters into frozen models, drastically reducing required parameters while maintaining performance.\n",
    "arXiv\n",
    "\n",
    "QLoRA builds on this by applying 4-bit quantization to reduce memory footprint further, enabling fine-tuning for large models even on hardware-limited setups. QLoRA retains near full-performance while being extremely efficient\n",
    "\n",
    " We'll use PEFT with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# Load model in float16 or bf16 if possible, or fallback to fp32\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsr-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
